{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95289d65-367f-4225-a28d-339a31d37d1a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Lab: Used Car Data Analysis\n",
    "---\n",
    "In this lab, we will use the UsedCars.csv to practice ETL operations on data in Azure Databricks, create a dashboard and execute a simple ML regression on our data. This will all be done by using our Containers in the Blob Storage and through the Secrets CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f96727df-49ea-4278-af64-96490fca08ae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Connect to the Storage\n",
    "\n",
    "### 1. Mount a new Blob Container  \n",
    "The container should be have one file uploaded to it. The file is included in the Workshop folder and it is called: UsedCars.csv.  \n",
    "  \n",
    "Fill in the values that are surrounded by `<>`.  \n",
    "- Enter the name of the Storage Account where it says `< YOUR STORAGE ACCOUNT NAME >`  \n",
    "  - If your storage account name is `datastore` then the code should be `storage_account = 'datastore'`  \n",
    "- Enter the name of the container in the Storage Account where it says `< YOUR CONTAINER NAME >`  \n",
    "  - If the name of the contianer is `mydata` then the code should say `container = 'mydata'`  \n",
    "- Enter the SAS token that was generated for this Demo where it sas `< YOUR SAS TOKEN >`  \n",
    "  - If the value of the SAS token is `3ij983jf3j02jtgeo-0fk23jf048fhwf` than the code should say `blobKey = '3ij983jf3j02jtgeo-0fk23jf048fhwf'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "149be158-05e1-43ea-94d0-4123b6bbcf88",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT EDIT \n",
    "# The below function will handle mounting for us. We will call it in the next cell.\n",
    "def mountblobstorage(storage_account, container, blobKey, mnt_location, blobEndpoint):\n",
    "  try:\n",
    "    dbutils.fs.mount(\n",
    "      source = blobEndpoint,\n",
    "      mount_point = mnt_location,\n",
    "      extra_configs = {\"fs.azure.sas.{1}.{0}.blob.core.windows.net\".format(storage_account, container):blobKey})\n",
    "  except Exception as e:\n",
    "    print(e)\n",
    "    dbutils.fs.unmount(mnt_location)\n",
    "    mountblobstorage(storage_account, container, blobKey, mnt_location, blobEndpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97e4eaf0-3f21-484e-968d-1ecb3759d7c9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Wait a second... are you really pasting your credentials here without secrets management?\n",
    "\n",
    "Copying and pasting tokens and keys directly into Azure Databricks poses security risks and challenges. It's preferable to use **Azure Key Vault** or the **the creation of secrets through the CLI**. Storing credentials in code or notebooks increases the risk of accidental exposure. \n",
    "\n",
    "#### Azure Key Vault\n",
    "Azure Key Vault provides a secure storage for secrets, with robust access control and permission management. It allows centralized management, enabling easy updates and rotation of secrets without code changes. Key Vault offers auditing capabilities for compliance and integrates seamlessly with Azure Databricks. Utilizing Key Vault enhances security, access control, management, auditing, and flexibility in handling secrets.\n",
    "\n",
    "#### Secrets through the CLI\n",
    "The creation of secrets through the CLI offers a standardized and programmatic way to securely retrieve secrets during runtime, eliminating the need for hardcoded or exposed sensitive information. It enables integration with different development frameworks, enhancing versatility. By utilizing the Secrets API, you can benefit from advanced access control, auditing, and compliance features provided by the secrets store. This approach promotes best practices in security, simplifies secrets management, and ensures the protection of sensitive data in your applications.\n",
    "\n",
    "First, we will need to install or update the CLI in our local machines. Please follow the guide here [Databricks CLI](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/cli/)\n",
    "\n",
    "Secondly, we will need to create the secrets in our CLI. Follow the guide here [Secrets CLI](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/cli/secrets-cli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bccc5658-01f9-4879-9500-3a970bfbf194",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.unmount(\"/mnt/usedcars\")\n",
    "\n",
    "# FILL IN the below values denoted with <>\n",
    "storage_account = \"< YOUR STORAGE ACCOUNT NAME >\"\n",
    "container = \"< YOUR CONTAINER NAME >\"\n",
    "blobKey = \"< YOUR SAS TOKEN >\"\n",
    "\t\n",
    "mnt_location = \"/mnt/usedcars\"\n",
    "blobEndpoint = \"wasbs://{1}@{0}.blob.core.windows.net/\".format(storage_account, container)\n",
    "mountblobstorage(storage_account, container, blobKey, mnt_location, blobEndpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1ec5d06-3c4d-4772-af67-df71239c4b3a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Step 2: Check the contents of the mounted storage  \n",
    "Use the `dbutils.fs` utility to check the contents of the newly mounted Blob Container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2837db92-3a52-4699-9c0b-b544e133ffe8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"/mnt/usedcars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1446ced-8a5b-4733-a362-8bebd4e9006e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Step 3: Take a Look at the Data  \n",
    "We can take a look at the contents of the csv file by invoking head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee40b3b3-4a17-44ef-8585-de74134bd69e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs head \"/mnt/usedcars/UsedCars.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d46ba1ab-ebe9-4b21-a3cf-33d8176fa4da",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Structure our Data  \n",
    "A best practice is to explicitly define a schema when loading our data into a dataframe. This improves our execution time by reducing the number of jobs submitted to our cluster.  \n",
    "Fill in the missing code where it says `<FILL IN>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22fb10f8-dc31-43d4-8993-14644476d565",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#FILL IN\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "UsedCarSchema = StructType([StructField(\"Index\", IntegerType(), True),\n",
    "                     StructField(\"Price\", FloatType(), True),\n",
    "                     StructField(\"Age\", FloatType(), True),\n",
    "                     StructField(\"KM\", FloatType(), True),\n",
    "                     StructField(\"FuelType\", StringType(), True),\n",
    "                     StructField(\"HP\", IntegerType(), True),\n",
    "                     StructField(\"MetColor\", IntegerType(), True),\n",
    "                     StructField(\"Automatic\", IntegerType(), True),\n",
    "                     StructField(\"CC\", IntegerType(), True),\n",
    "                     StructField(\"Doors\", IntegerType(), True),\n",
    "                     \"<FILL IN>\"\n",
    "                     ])\n",
    "                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b598cf3c-2fc3-4b9b-a37b-da212bd2403c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Compare the Execution Times between `inferSchema` and Passing a `StructType` schema object  \n",
    "\n",
    "When working with structured data in Apache Spark, you have two options for defining the schema of your data: using inferSchema or passing a StructType schema object. Let's compare the execution times of these two approaches:\n",
    "\n",
    "- **inferSchema**: The inferSchema method automatically infers the schema of the data by sampling a portion of the dataset. It reads the data and analyzes the values in each column to determine the data types. While this approach is convenient and requires minimal code, it can be slower for large datasets as it needs to read and analyze a sample of the data. The execution time for inferSchema depends on the size of the sample and the complexity of the data.\n",
    "\n",
    "- **StructType schema object**: Alternatively, you can explicitly define the schema using a StructType object. This approach requires you to specify the data types and structure of each column in your dataset. While it involves writing more code upfront, it provides precise control over the schema and can be faster for large datasets. The execution time for passing a StructType schema object is typically faster as Spark does not need to infer the schema by analyzing the data.\n",
    "\n",
    "Take a look at the execution time for our two implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "977e1b55-36f1-445d-b6bd-717c5b9713e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%timeit df = spark.read.format(\"csv\").schema(UsedCarSchema).option(\"header\", True).load(\"/mnt/usedcars/UsedCars.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d09435be-4ed4-4bc1-99d5-7537ca897f99",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%timeit dfInferred = spark.read.format(\"csv\").option(\"inferschema\", True).option(\"header\", True).load(\"/mnt/usedcars/UsedCars.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "867738d5-0c64-4d51-b4cf-0c15044a3648",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "By explicitly defining our schema, we were able to improve performance by reducing number of jobs submitted to our cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61b925c4-59a7-4cec-9714-858051c664cb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Data Transformation\n",
    "### Begin Transforming DataFrame  \n",
    "Let's start transforming our DataFrame. First, we need to read in the data with the schema object defined above.  \n",
    "Fill in the missing code where it says `<FILL IN>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d391b6b6-caa9-465f-b6d3-e894ff416520",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#FILL IN\n",
    "df = \"<FILL IN>\"\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0456bd32-b959-4b03-b1a1-b06f7d15de83",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As we may see on the table above, databricks already has a built-in `Index` so we don't need to have a column with that information. Let's go ahead and drop that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b8a5ea7-1b06-46fb-b6c2-e7b48f4656cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop(\"Index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfa12171-3494-441f-9be1-29c6ae4c1417",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Eliminate Duplicates  \n",
    "We need to clean up our `FuelType` column to eliminate duplicate values.\n",
    "Let's start by taking a look at all the fuel types that we have on our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e02a622-f2ac-4e3a-b510-3bbe2e492ab9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#FILL IN\n",
    "display(df.select(\"FuelType\").\"<FILL IN>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78974bf1-a327-4b97-88bf-a0ca27eff148",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "If cars are your specialty, you might know that Compressed natural gas can also be called \"methane\". At the same time, in our dataset, we can see that we have the abbreviation \"CNG\". \n",
    "\n",
    "Let's do some data cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce615f8e-05dd-4642-8155-ea6a28a0d74f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.na.replace([\"Diesel\", \"Petrol\", \"CNG\", \"methane\", \"CompressedNaturalGas\"], [\"diesel\", \"petrol\", \"cng\", \"cng\", \"cng\"], \"FuelType\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61dea31e-e6e5-4807-aa3d-2b549256e783",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In this case, we are only handling the duplicate values for `FuelType`, however, it's a good practice to analyze all columns in your dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a7e3d22-6ea6-4e61-bbed-41dc36d09cb7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Handling `null` values  \n",
    "There mht be null values represented in our dataset. It's important to remove null values as they can hinder the accuracy and reliability of data analysis and machine learning models.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "If there are any, let's drop any rows containing null values.\n",
    "\n",
    "**Hint**: [Stackoverflow](https://stackoverflow.com/questions/61499910/alias-in-pyspark) is always our friend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "595c0890-a15e-407e-a0cf-0b3ce7b3ed92",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#First let's check the null values\n",
    "\"<FILL IN>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12113956-2740-4237-a2c5-9f13b2db689c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6974ab7-1d56-43dc-8527-a82a049991e7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Global Managed Table  \n",
    "\n",
    "In Databricks, a global managed table is a type of table that is stored and managed in a global metastore, allowing it to be accessed across different Databricks workspaces and clusters within an Azure Databricks account.\n",
    "When a table is created as a global managed table, its metadata and schema are stored in the global metastore, while the actual data resides in a distributed file system, such as Azure Data Lake Storage. This enables seamless sharing and collaboration across different workspaces and clusters within the same Databricks account.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "As we have finished our data cleaning, and we want the rest of our company to have access to this beautifully cleaned table, let's go ahead and create a global managed table from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d47da751-4e56-41b4-b7b3-f0d828cab8c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").saveAsTable(\"usedcars_cleaned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b74b9ef4-795c-4f2e-bb1c-9cc7f25093b6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Data Exploration \n",
    "Explore the data writing Spark SQL queries on our newly created table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7f7adcd-c3dc-4cc0-8fd7-c6b6c64439bd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65c1d2d9-cffb-47d7-90ef-9fc0912629ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from usedcars_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecccbd22-7857-4843-991c-2bc4a88481f6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Query 2\n",
    "Introduce a new table containing data for transmission type. We will join it with usedcars_cleaned table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eebf51f1-b9a7-4275-ac11-85e9af744cab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql create table if not exists car_transmission (transmission_id int, transmission_type string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7430cfdc-7b38-4055-a1c5-ea42b5a7a74e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "insert overwrite table car_transmission values (0, \"Manual\"), (1, \"Automatic\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8c1c279-9302-4bf3-a7c3-fe5d112c2931",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql select * from car_transmission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af1f74d3-91fb-43a9-ae63-9f75bf92e094",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Query 3\n",
    "\n",
    "Let's check the number of cars that have each transmission type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ec1d567-6b7d-4901-8087-fc78d8bfe6df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select \"<FILL IN>\", count(\"<FILL IN>\") as cnt\n",
    "from usedcars_cleaned join car_transmission on usedcars_cleaned.Automatic = car_transmission.transmission_id\n",
    "group by transmission_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f199fb3-613e-4d87-8bf0-4a3feeb4219c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Dashboard\n",
    "The following queries produce some interesting insights into the data. Try Databricks' visuals, and output the results in a bar or pie chart. Get to know more on [Visualization in Databricks](https://learn.microsoft.com/en-us/azure/databricks/visualizations/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bb09eb3-e650-40bf-a4b7-1092730f4c02",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Number of Cars Available\n",
    "We firstly need to analyze how many cars we have based on its `FuelType`. We will summarize this information on a pie chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f4974d3-e555-4a79-be04-7b69ea6e8f1a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select \"<FILL IN>\", count(\"<FILL IN>\")\n",
    "from usedcars_cleaned\n",
    "group by \"<FILL IN>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0afab3a1-6fb8-4c20-b875-9e423ff7e963",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Car Prices based on Fuel Type\n",
    "Our company needs to have information on the average price of the cars based on their `FuelType`. Let's make a bar chart based on that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3feec132-bf3c-4398-9a5c-aa28d554cbd9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select \"<FILL IN>\", avg(Price) as AvgPrice\n",
    "from usedcars_cleaned\n",
    "group by FuelType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fb243d1-b0c8-497c-9da3-71297a6ccca8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Cars Longevity\n",
    "What type of cars might we be selling? Do we have older cars (or the so called *classics*) or do we have some new-second-hand cars? Let's have a look at that, in a line chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "590561e4-5fb5-4b54-adba-2d996fce95b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select Age, count(\"<FILL IN>\")\n",
    "from usedcars_cleaned\n",
    "group by Age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e089cdd8-924a-46d6-aead-caf30c92eb00",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Kilometers Driven\n",
    "\n",
    "We might think there is a connection between the number of km driven in a car with it's average price. Let's check that on a bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f46a4f57-f7bc-4896-b1e3-74d35a98d83d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select t.range as km_driven, avg(Price) as avgPrice\n",
    "from (select case  \n",
    "    when KM between 0 and 49999 then '0 < x < 50k'\n",
    "    when KM between 50000 and 99999 then '50k <= x < 100k'\n",
    "    else 'x >= 100k' end as range, Price\n",
    "  from usedcars_cleaned) t\n",
    "group by t.range\n",
    "order by km_driven"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c194a117-c322-4647-b557-0f86414ee7ee",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Price distribution\n",
    "\n",
    "When we are looking at the price of our cars, let's see how they are distributed in a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dee930d7-e2a4-48e6-8b9e-1be0bb16706f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select Price, count(\"<FILL IN>\")\n",
    "from usedcars_cleaned\n",
    "group by \"<FILL IN>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bbe8a11-0af4-4b21-b4a5-a2d526e2e79a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### DYI Query\n",
    "\n",
    "Feel free to imagine something you would want to know! Try writing a Spark SQL query of your own to explore the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05a0d27e-72d6-479d-a5ca-f398e55381c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- \"<FILL IN>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d278515-6ff4-4b13-a2c5-8f154394baf6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now it's time to... **create your own dashboard**! Using the knowledge you acquired on the 5th lab of this workshop, let's create a dashboard with these 5 visualizations that we have created. Adapt it however you want, and in the end take a printscreen and share with the rest of the class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93431ebb-f3eb-467a-a3cd-d459d8c09e20",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Machine Learning \n",
    "\n",
    "It's time to start our machine learning project aimed at predicting the price of used cars. With a focus on providing accurate and reliable pricing estimates, our company is leveraging a cleaned dataset containing valuable information on various attributes of used cars. \n",
    "\n",
    "By utilizing this dataset, we aim to build a robust machine learning pipeline that can effectively analyze the features of a used car and **provide an estimated price** based on historical data. This project will help to assist our sellers in making informed decisions (for example, on the pricing of a *new* car added to our website) by offering insights into the fair market value of used cars, thereby enhancing transparency and facilitating fair transactions in the automotive marketplace.\n",
    "\n",
    "The normal steps of a machine learning project are:\n",
    "\n",
    "1. **Data Cleaning** - We've already worked on this by cleaning duplicate values and making sure there aren't any null values\n",
    "2. **Exploratory Data Analysis (EDA)** - We've already seen our data, visualizing most of the important feature, identifying patterns and some important correlations.\n",
    "3. **Feature Engineering** - We are going to start here! In order to do this, we should select the features (columns) that bring the most value to predict the price of a car. The simplest way we can do this, is by keeping only the variables that have continuous values (integer or floats)\n",
    "4. **Split our data** - dividing a dataset into separate training and testing subsets, allowing the machine learning model to be trained on the training set and evaluated on the testing set to assess its performance on unseen data.\n",
    "5. **Model Selection** - We are going to do a simple Regression\n",
    "6. **Model Evaluation** - Let's finish by checking the performance of our models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cf2a4c6-7bfe-45e5-bb33-b42cabd80e64",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Feature Engineering\n",
    "Let's remove the columns that are not of the type string and have continuous values: Price, Age, KM and the Weight of the Car\n",
    "\n",
    "**Hint**: On the first fill in you will need to use the table that already has a schema defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b963b71a-6ba0-4de5-b231-18a8079ba6a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "non_string_columns = [column for column in df.columns if \"<FILL IN>\".schema[column].dataType != StringType()]\n",
    "df_filtered = \"<FILL IN>\"\n",
    "\n",
    "display(df_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd6f64d7-bb5b-499b-b49a-8b807a3d20db",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Split our data\n",
    "Now we have the data ready for the prediction, let's separate our dataset to put it to work. In the cell below you won't have any action required. We will separate our dataset first into the *X* dataset, that will have every column but the price, and the *y* that will have the information on the price. \n",
    "\n",
    "Then we will follow onto separating our dataset into 2 subsets:\n",
    "1. The **training set** is used to teach the machine learning model how to make predictions based on the other columns. It will help the model learn the underlying patterns and relationships in the data.\n",
    "\n",
    "2. The **testing set** is used to assess how well the trained model performs on new, unseen data. It helps measure the model's accuracy and see how effectively it can predict or classify the target variable for data it has not seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63c3d9e0-2837-464a-bfb0-b3bdf5ab30c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Separate the input features (X) and the target variable (y)\n",
    "X = \"<FILL IN>\"  # DataFrame with input features\n",
    "y = \"<FILL IN>\"  # DataFrame with target variable\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test = X.randomSplit([0.7, 0.3], seed=12345)\n",
    "y_train, y_test = y.randomSplit([0.7, 0.3], seed=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bd35dc9-d68e-442d-b7fa-dcf0a6de7225",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36206184-a5fe-4965-9e92-d44fa9135850",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Separate the input features (X) and the target variable (y)\n",
    "X = df_filtered.drop('price')  # DataFrame with input features\n",
    "y = df_filtered.select('price')  # DataFrame with target variable\n",
    "\n",
    "# Create a VectorAssembler to assemble the input features into a single vector column\n",
    "assembler = VectorAssembler(inputCols=X.columns, outputCol='features')\n",
    "X_assembled = assembler.transform(X)\n",
    "\n",
    "# Add the target variable column to the assembled data\n",
    "assembled_data = X_assembled.join(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test = assembled_data.randomSplit([0.7, 0.3], seed=12345)\n",
    "\n",
    "# Create a Linear Regression model\n",
    "lr = LinearRegression(featuresCol='features', labelCol='price', regParam=0.0)\n",
    "\n",
    "# Fit the model to the training data\n",
    "lr_model = lr.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "080c2b0a-70ca-4bc0-9e69-09cc0f9a8587",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7006700-33c7-48ce-8bce-f5b794f82d50",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "predictions = lr_model.transform(X_test)\n",
    "\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))\n",
    "\n",
    "# Use the model to make predictions on the testing data\n",
    "predictions = lr_model.transform(X_test)\n",
    "\n",
    "predictions.show()  "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1783283202011594,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "fulllabfile",
   "widgets": {}
  },
  "name": "M03_L01_Lab01",
  "notebookId": 1897729898649988
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
